# Configuration file for LSTM-Attention-SHAP Volatility Forecasting
# All hyperparameters and settings in one place

[DATA]
# Data parameters
start_date = 2018-01-01
end_date = 2024-12-31
lookback_window = 30
train_end_date = 2022-12-31
val_end_date = 2023-06-30

# Asset selection (for real data)
primary_ticker = SPY
alternative_tickers = QQQ,GLD,USO,UUP

[FEATURES]
# Feature engineering
rolling_window = 30
standardization_window = 252
lag_periods = 1,5,22

# Feature list
price_features = returns,high_low_spread
volume_features = volume_normalized
volatility_features = realized_volatility,vix
risk_features = gpr_index
lagged_features = rv_lag1,rv_lag5,rv_lag22,returns_lag1,returns_lag5,returns_lag22

[MODEL]
# LSTM-Attention Architecture
lstm_units = 128,64
attention_units = 64
dense_units = 32
dropout = 0.2
recurrent_dropout = 0.1

# Output heads
volatility_activation = linear
var_activation = linear
var_quantile = 0.01

[TRAINING]
# Training parameters
epochs = 100
batch_size = 64
learning_rate = 0.001
beta_1 = 0.9
beta_2 = 0.999

# Loss weights
volatility_loss_weight = 0.7
var_loss_weight = 0.3

# Callbacks
early_stopping_patience = 15
reduce_lr_patience = 5
min_delta = 0.0001
lr_reduction_factor = 0.5

[EVALUATION]
# Evaluation metrics
metrics = RMSE,MAE,QLIKE,R2

# VaR backtesting
var_confidence = 0.99
var_alpha = 0.01
significance_level = 0.05

[EXPLAINABILITY]
# SHAP analysis
shap_background_samples = 100
shap_test_samples = 200
clustering_method = kmeans
n_clusters = 100

# Attention visualization
attention_heatmap_samples = 60

[BASELINE_MODELS]
# GARCH parameters
garch_p = 1
garch_q = 1

# EGARCH parameters
egarch_p = 1
egarch_o = 1
egarch_q = 1

# HAR-RV parameters
har_daily_lag = 1
har_weekly_lags = 5
har_monthly_lags = 22

# Historical volatility
hist_vol_window = 30

[PATHS]
# Directory paths (relative to project root)
data_dir = data
models_dir = models
figures_dir = figures
tables_dir = tables
tests_dir = tests

[OUTPUT]
# Output settings
figure_dpi = 300
figure_format = png
table_format = csv
save_training_history = true
save_attention_weights = true
save_shap_values = true

[REPRODUCIBILITY]
# Random seeds for reproducibility
numpy_seed = 123
tensorflow_seed = 456
python_seed = 42
